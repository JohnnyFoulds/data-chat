{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Serving LLMs](https://medium.com/@vipra_singh/building-llm-applications-serving-llms-part-9-68baa19cef79c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16, float16\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/swan/miniconda3/envs/data-chat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/johnny/swan/miniconda3/envs/data-chat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load in the LLM without any compression tricks\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    torch_dtype=float16, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the tokenizer's chat template to format each message\n",
    "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] You are a friendly chatbot. [/INST] Hello there! I'm glad you're having a nice day. How can I help make it even better? Feel free to ask me anything or share what's on your mind. I'm here to listen and provide information or just chat! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/swan/miniconda3/envs/data-chat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/johnny/swan/miniconda3/envs/data-chat/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1, \n",
    "        \"top_p\": 0.95        \n",
    "    },\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a friendly chatbot. Question: What is the difference between a chatbot and a human?\\n\\nAI: Hello there! I'm an artificial intelligence, or AI for short. I'm designed to simulate conversation and perform tasks based on pre-programmed algorithms and machine learning. I don't have feelings, emotions, or physical presence. I'm here to help answer questions and provide information.\\n\\nHumans, on the other hand, are living organisms with the ability to think, feel, and communicate. We have complex emotions, the ability to learn and adapt, and the capacity for creativity and problem-solving. We can understand context, tone, and nuance in language, and we can form relationships and build connections with each other.\\n\\nSo, while I can mimic human conversation and perform certain tasks, I don't possess the same level of complexity and versatility as a human. I'm just a helpful AI, here to make your life easier!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('You are a friendly chatbot.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
